#!/bin/bash

# COLORS
RESET="\e[0m"
BLACK="\e[30m"
RED="\e[31m"
GREEN="\e[32m"
YELLOW="\e[33m"
BLUE="\e[34m"
MAGENTA="\e[35m"
CYAN="\e[36m"
WHITE="\e[37m"


RECURSIVE=false
MAX_DEPTH=5
WGET_PATH="./data/"
URL=""

# parse options
parse() {
	while [[ $# -gt 0 ]]; do		# $# is the number of args, that decreases when using shift
		if [[ $1 == -* ]]; then		# if arg starts with '-'
			arg="$1"
			for (( i=1; i <${#arg}; i++ )); do	# for each char in arg (skipping '-')
				if [[ ${arg:i:1} == 'r' ]]; then
					RECURSIVE=true
				elif [[ ${arg:i:1} == 'l' ]]; then
					if [[ -n $2 && $2 =~ ^[0-9]+$ ]]; then		# -n $2 checks if $2 is not empty
						MAX_DEPTH="$2"
						shift
					else
						MAX_DEPTH=5
					fi
				elif [[ ${arg:i:1} == 'p' ]]; then
					if [[ -n $2 ]]; then
						WGET_PATH="$2"
						shift
					fi
				else
					echo "unknown argument '${arg:i:1}'"
					exit
				fi
			done
			shift
		else
			if [[ "$URL" == "" ]]; then
				URL=$1
				shift
			else
				echo "too many arguments"
				exit
			fi
		fi
	done
}



crawl() {
	local	url=$1
	local	depth=$2

	if [[ $depth -gt MAX_DEPTH ]]; then
		return
	fi

	echo -e "$MAGENTA Crawling in $url $RESET"

	download_images $url

	if [[ "$RECURSIVE" == true && $depth -lt $MAX_DEPTH ]]; then
		# find hyperlinks
		curl --silent $url |			# curl the page
		grep -o '<a[^>]*href=[^>]*>' |	# get the hl lines
		awk -F 'href=' '{print $2}' |	# split on href= and get second element
		awk -F '"' '{print $2}' |		# split on " and get second element
		while read -r link; do
			[[ "$link" =~ ^https?:// ]] || link="${URL%/}/$link"
			echo -e "$YELLOW  found link : $link$RESET"
			crawl $link $((depth + 1))
		done
	fi
}

download_images() {
	local	url=$1

	curl --silent $url |				# curl the page
	grep -o '<img[^>]*src=[^>]*>' |		# get the img lines
	awk -F 'src=' '{print $2}' |		# split on src= and get second element
	awk -F '"' '{print $2}' |			# split on " and get second element
	grep -E '*\.(jpg|jpeg|png|bmp|gif)$' |	# filter filetype
	while read -r img; do
		[[ "$img" =~ ^https?:// ]] || img="${URL%/}/$img"
		echo -e "$GREEN  Dowloading: $img$RESET"
		wget --quiet -P $WGET_PATH "$img"
	done
}

parse "$@"
crawl $URL 0
